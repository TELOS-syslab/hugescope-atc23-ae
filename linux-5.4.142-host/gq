[1mdiff --git a/arch/x86/kvm/page_migration.c b/arch/x86/kvm/page_migration.c[m
[1mindex 9cfd0359b..59964f44e 100644[m
[1m--- a/arch/x86/kvm/page_migration.c[m
[1m+++ b/arch/x86/kvm/page_migration.c[m
[36m@@ -9,6 +9,7 @@[m
 #include <linux/mm.h>[m
 #include "ss_work.h"[m
 #include "vmx/vmx.h"[m
[32m+[m[32m#include "free_page.h"[m
 [m
 extern struct kvm* kvm_global;[m
 extern int isolate_lru_page(struct page *page);[m
[36m@@ -32,10 +33,17 @@[m [mint dirty_num;[m
 [m
 struct page_node* pages;[m
 int __page_index;[m
[32m+[m[32mint pm_dram_index;[m
[32m+[m[32mint pm_nvm_index;[m
[32m+[m
[32m+[m[32m#include "linux_pm.h"[m
 [m
 void init_pm_list(void){[m
         if(!pages)[m
                 pages = (struct page_node*)vmalloc(sizeof(struct page_node)*PAGE_NUM);[m
[32m+[m	[32m__page_index = 0;[m
[32m+[m	[32mpm_dram_index = 0;[m
[32m+[m	[32mpm_nvm_index = 0;[m
 }[m
 [m
 void init_page_migration(){[m
[36m@@ -57,6 +65,7 @@[m [mvoid close_page_migration(){[m
 	pm_pml_buffer = NULL;[m
 }[m
 [m
[32m+[m
 void get_pml_log_for_page_migration(struct kvm_vcpu* vcpu,ul gpa){[m
 	ul gfn = gpa>>12;[m
 	if(gfn<PAGE_NUM){	[m
[36m@@ -161,12 +170,13 @@[m [mint copy_pages(void* data){[m
 	struct page_node* pages;[m
 	struct kvm_vcpu *vcpu;[m
 	int rc;[m
[32m+[m	[32mint new_pages = 0;[m
 [m
 	p = (struct page_list*) data;[m
 	pages = p->pages;[m
 	size = p->size;[m
  [m
[31m-	//printk("alloc new pages\n");[m
[32m+[m	[32m//printk("thread_id %d: alloc new pages\n",p->id);[m
 	for(i=0;i<size;i++){[m
 		    [m
 		node = pages[i].target_node;[m
[36m@@ -193,7 +203,12 @@[m [mint copy_pages(void* data){[m
 		}[m
 		[m
 		//alloc a new page[m
[31m- 	        new = __alloc_pages_node(node, GFP_HIGHUSER_MOVABLE| __GFP_THISNODE, 0);[m
[32m+[m		[32m//new = get_free_page_from_target_cache(node);[m
[32m+[m		[32m//if(!new){[m
[32m+[m			[32mnew = __alloc_pages_node(node, GFP_HIGHUSER_MOVABLE| __GFP_THISNODE, 0);[m
[32m+[m		[32m//	new_pages++;[m
[32m+[m[32m//			//printk("pool page is not enough %d\n",node);[m
[32m+[m		[32m//}[m
 		pages[i].new = new;	[m
 [m
 		//lock new page[m
[36m@@ -212,6 +227,7 @@[m [mint copy_pages(void* data){[m
 		if(PageAnon(old) && !PageKsm(old))[m
 			pages[i].anon_vma = page_get_anon_vma(old);[m
 	}[m
[32m+[m	[32m//printk("thread_id %d: alloc new pages finished\n",p->id);[m
 	[m
 	//printk("flush ept dirty bits\n");[m
 	for(i=0;i<size;i++){[m
[36m@@ -301,6 +317,7 @@[m [mint copy_pages(void* data){[m
                 }[m
         }[m
 [m
[32m+[m	[32m//printk("not enough %d\n",new_pages);[m
 	printk("copy-data thread %d  page num %d\n",atomic_read(&v),size);[m
 	atomic_inc(&v);[m
 [m
[36m@@ -341,6 +358,7 @@[m [mvoid begin_migration(struct page_node *pages, int size){[m
 			tsize-=num;[m
 		}[m
 		else {pl->size = tsize;tsize=0;}[m
[32m+[m		[32mpl->id = i;[m
 		kthread_run(copy_pages,(void*)pl,"child-%d",i);[m
 		printk("child thread %d\n",i);[m
 		thread_num++;[m
[36m@@ -354,6 +372,7 @@[m [mvoid begin_migration(struct page_node *pages, int size){[m
 	printk("after parallel\n");[m
 [m
 	printk("dirty page num %d\n",dirty_num);[m
[32m+[m
 	print_migration_log(pages,size);[m
 }[m
 EXPORT_SYMBOL_GPL(begin_migration);[m
[36m@@ -392,132 +411,6 @@[m [mvoid print_migration_log(struct page_node* pages,int size){[m
 	printk("err: cannot move mapping %d\n",err4);			[m
 }[m
 [m
[31m-extern struct page *alloc_new_node_page(struct page *page, unsigned long node);[m
[31m-extern int move_to_new_page(struct page *newpage, struct page *page,enum migrate_mode mode);[m
[31m-[m
[31m-void __begin_migration(struct page_node* pages,int size){[m
[31m-        int node = 0;[m
[31m-        int err;[m
[31m-        int i;[m
[31m-	int fail_num = 0;[m
[31m-        int rc;[m
[31m-	bool is_lru;[m
[31m-        struct anon_vma *anon_vma = NULL;[m
[31m-        struct page *page;[m
[31m-        struct page *newpage;[m
[31m-        struct address_space *mapping;[m
[31m-[m
[31m-[m
[31m-        LIST_HEAD(plist);[m
[31m-	migrate_prep();[m
[31m-	size = size;[m
[31m-        printk("size %d\n",size);[m
[31m-        for(i=0;i<size;i++){[m
[31m-		page = pfn_to_page(pages[i].pfn);[m
[31m-                err = isolate_lru_page(compound_head(page));[m
[31m-                page = compound_head(page);[m
[31m-		[m
[31m-		is_lru = !__PageMovable(page);[m
[31m-                //for freed page.. no use[m
[31m-                if (page_count(page) == 1) {[m
[31m-                        printk("page %#lx is free\n",pages[i].pfn);[m
[31m-                        ClearPageActive(page);[m
[31m-                        ClearPageUnevictable(page);[m
[31m-                        if (unlikely(__PageMovable(page))) {[m
[31m-                                lock_page(page);[m
[31m-                                if (!PageMovable(page))[m
[31m-                                        __ClearPageIsolated(page);[m
[31m-                                unlock_page(page);[m
[31m-                        }[m
[31m-                        continue;[m
[31m-                }[m
[31m-[m
[31m-                //lock old page[m
[31m-                if (!trylock_page(page)){[m
[31m-                      lock_page(page);[m
[31m-		      continue;[m
[31m-                }[m
[31m-[m
[31m-		newpage = __alloc_pages_node(node, GFP_HIGHUSER_MOVABLE|__GFP_THISNODE, 0);[m
[31m-[m
[31m-		//write for writeback[m
[31m-                if (PageWriteback(page)){[m
[31m-                        wait_on_page_writeback(page);[m
[31m-                }[m
[31m-                //delays freeing anon_vma pointer until the end of migration.[m
[31m-                if (PageAnon(page) && !PageKsm(page))[m
[31m-                        anon_vma = page_get_anon_vma(page);[m
[31m-[m
[31m-                //lock new page[m
[31m-                if(unlikely(!trylock_page(newpage)))[m
[31m-                        goto out_unlock;[m
[31m-[m
[31m-                //no idea[m
[31m-                if (unlikely(!is_lru)) {[m
[31m-                        printk("!is_lru\n");[m
[31m-                        rc = move_to_new_page(newpage, page, MIGRATE_SYNC);[m
[31m-                        goto out_unlock_both;[m
[31m-                }[m
[31m-                //unamp all mapping of old page[m
[31m-                try_to_unmap(page,TTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);[m
[31m-[m
[31m-                mapping = page_mapping(page);[m
[31m-[m
[31m-                if (!mapping){[m
[31m-                        //      printk("!mapping\n");[m
[31m-                        rc = migrate_page(mapping, newpage, page, MIGRATE_SYNC);[m
[31m-[m
[31m-                }else if (mapping->a_ops->migratepage){[m
[31m-[m
[31m-                        printk("mapping->a_ops\n");[m
[31m-                        rc = mapping->a_ops->migratepage(mapping, newpage,[m
[31m-                                                        page, MIGRATE_SYNC);[m
[31m-                }else{[m
[31m-                        printk("fallback\n");[m
[31m-                }[m
[31m-[m
[31m-                if (rc == MIGRATEPAGE_SUCCESS) {[m
[31m-                        if (__PageMovable(page))[m
[31m-                                __ClearPageIsolated(page);[m
[31m-[m
[31m-                        if (!PageMappingFlags(page))[m
[31m-                                page->mapping = NULL;[m
[31m-[m
[31m-                        if (likely(!is_zone_device_page(newpage)))[m
[31m-                                flush_dcache_page(newpage);[m
[31m-                }[m
[31m-		[m
[31m-		//remove the mapping of old page to new page[m
[31m-                remove_migration_ptes(page,rc == MIGRATEPAGE_SUCCESS ? newpage : page, false);[m
[31m-[m
[31m-out_unlock_both:[m
[31m-                unlock_page(newpage);[m
[31m-out_unlock:[m
[31m-                if (anon_vma)[m
[31m-                        put_anon_vma(anon_vma);[m
[31m-[m
[31m-                unlock_page(page);[m
[31m-[m
[31m-                if (rc == MIGRATEPAGE_SUCCESS) {[m
[31m-                        put_page(page);[m
[31m-[m
[31m-                        if (unlikely(!is_lru))[m
[31m-                                put_page(newpage);[m
[31m-                        else[m
[31m-                                putback_lru_page(newpage);[m
[31m-                }else{[m
[31m-                        fail_num++;[m
[31m-                        put_page(newpage);[m
[31m-                }[m
[31m-	[m
[31m-	}[m
[31m-[m
[31m-        printk("sucess_num %d fail_num %d\n",size-fail_num,fail_num);[m
[31m-        printk("sucess_Mem %d MB, fail_Mem %d MB\n",(size-fail_num)*4/1024,fail_num*4/1024);[m
[31m-        return;[m
[31m-}[m
[31m-[m
[31m-[m
 struct kvm_rmap_head *__gfn_to_rmap(gfn_t gfn, struct kvm_memory_slot *slot){[m
         unsigned long idx;[m
         idx = gfn_to_index(gfn, slot->base_gfn, PT_PAGE_TABLE_LEVEL);[m
[36m@@ -551,7 +444,7 @@[m [mvoid gfn_to_pagenode(struct page_node* node,ul gfn){[m
 		node->sptep = (ul)sptep;[m
 		node->spte = (ul)*sptep;[m
 		node->rc = 0;[m
[31m-		node->target_node = 2;[m
[32m+[m		[32mnode->target_node = 0;[m
 		node->current_node = pfn_to_nid(pfn);[m
 		if(node->target_node==node->current_node)[m
 			return;[m
[36m@@ -577,7 +470,14 @@[m [mvoid migration(void){[m
 void insert_pm_page(struct page_node p,int tar_node){[m
 	struct page_node* page;[m
 [m
[31m-	page = pages+__page_index; 	[m
[32m+[m	[32mif(tar_node==0){[m
[32m+[m		[32mpage = pages+dram_index;[m
[32m+[m	[32m        pm_dram_index+=2;[m[41m	[m
[32m+[m	[32m}else{[m
[32m+[m		[32mpage = pages+nvm_index;[m[41m [m
[32m+[m		[32mpm_nvm_index+=2;[m
[32m+[m	[32m}[m[41m		[m
[32m+[m	[32m__page_index++;[m[41m [m
 [m
 	page->gfn = p.gfn;[m
         page->pfn = p.pfn;[m
[36m@@ -586,7 +486,6 @@[m [mvoid insert_pm_page(struct page_node p,int tar_node){[m
         page->rc = 0;[m
         page->target_node = tar_node;[m
         page->current_node = p.current_node;[m
[31m-        __page_index++;[m
 [m
 	return;	[m
 }[m
[36m@@ -594,9 +493,9 @@[m [mvoid insert_pm_page(struct page_node p,int tar_node){[m
 [m
 [m
 /**************************************************************************/[m
[31m-void check_page_nid(struct kvm_vcpu* vcpu,ul buf_addr,ul max_index){};[m
 [m
[31m-/*[m
[32m+[m
[32m+[m[32mul* gfn_hash=NULL;[m
 [m
 void get_vm_all_gfns(void){[m
          struct kvm_memslots * memslots;[m
[36m@@ -645,22 +544,70 @@[m [mvoid get_vm_all_gfns(void){[m
 [m
 }[m
 [m
[32m+[m[32mvoid migrate_sys_pages(void){[m
[32m+[m	[32mint i;[m
 [m
[31m-void get_gfns(struct kvm_vcpu* vcpu,ul buf_addr,ul max_index){[m
[31m-	int i,j;[m
[31m-	ul gfn;[m
[31m-	ul gptp;[m
[31m-	ul host_addr;[m
[31m-	ul addr;[m
[31m-	ul __user* _user;[m
[31m-        __page_index = 0;[m
[31m-[m
[32m+[m[32m//	add_pm_free_pages();[m
[32m+[m	[32minit_pm_list();[m
[32m+[m[32m//	msleep(5000);[m
[32m+[m	[32mif(!gfn_hash)[m
[32m+[m		[32mgfn_hash = (ul*)vmalloc(sizeof(ul)*PAGE_NUM);[m
 	for(i=0;i<PAGE_NUM;i++)[m
 		gfn_hash[i]=0;[m
[31m-	[m
[32m+[m
[32m+[m	[32m__page_index = 0;[m[41m	[m
 	get_vm_all_gfns();[m
[32m+[m[41m	[m
[32m+[m	[32mkthread_run(pm,NULL,"page migrateion");[m[41m	[m
 [m
[32m+[m[32m}[m
[32m+[m
[32m+[m[32mvoid get_gfns(struct kvm_vcpu* vcpu,ul buf_addr, ul max_index){[m
[32m+[m	[32mint i,j;[m
[32m+[m[32m        ul gfn;[m
[32m+[m[32m        ul gptp;[m
[32m+[m[32m        ul host_addr;[m
[32m+[m[32m        ul addr;[m
[32m+[m[32m        ul __user* _user;[m
[32m+[m[32m        bool *temp = NULL;[m
[32m+[m
[32m+[m
[32m+[m	[32m//add_pm_free_pages();[m
[32m+[m[32m        init_pm_list();[m
[32m+[m[32m        //msleep(5000);[m
[32m+[m[32m        if(!gfn_hash)[m
[32m+[m[32m                gfn_hash = (ul*)vmalloc(sizeof(ul)*PAGE_NUM);[m
[32m+[m[32m        for(i=0;i<PAGE_NUM;i++)[m
[32m+[m[32m                gfn_hash[i]=0;[m
[32m+[m
[32m+[m[32m        host_addr = kvm_vcpu_gfn_to_hva_prot(vcpu,buf_addr>>12,temp);[m
[32m+[m[32m        for(i=0;i<max_index;i++){[m
[32m+[m[32m                _user = (ul __user *)((void *)host_addr +(i<<3));[m
[32m+[m[32m                if(__copy_from_user(&gptp, _user, sizeof(gptp))){[m
[32m+[m[32m                       printk("error\n");[m
[32m+[m[32m                       break;[m
[32m+[m[32m                }[m
[32m+[m[32m                //printk("gptp: %#lx\n",gptp);[m
[32m+[m[32m                addr = kvm_vcpu_gfn_to_hva_prot(vcpu,gptp,temp);[m
[32m+[m[32m                for(j=0;j<512;j++){[m
[32m+[m[32m                        _user = (ul __user *)((void *)addr +(j<<3));[m
[32m+[m[32m                        if(__copy_from_user(&gfn, _user, sizeof(gfn))){[m
[32m+[m[32m                             printk("error\n");[m
[32m+[m[32m                              break;[m
[32m+[m[32m                        }[m
[32m+[m[32m                        if((gfn&1)!=0){[m
[32m+[m[32m                                gfn = gfn & (((1ULL << 52) - 1) & ~(u64)(PAGE_SIZE-1));[m
[32m+[m[32m                                gfn = gfn>>12;[m
[32m+[m[32m                                if(gfn!=0&&!gfn_hash[gfn]){[m
[32m+[m[32m                                        gfn_hash[gfn]=1;[m
[32m+[m[32m                                        gfn_to_pagenode(&pages[__page_index],gfn);[m
[32m+[m[32m                                }[m
[32m+[m[32m                        }[m
[32m+[m
[32m+[m[32m                }[m
[32m+[m[32m        }[m
[32m+[m	[32mprintk("num is %lu\n",__page_index);[m[41m	[m
 	kthread_run(pm,NULL,"page migrateion");	[m
 [m
 }[m
[31m-*/[m
[41m+[m
